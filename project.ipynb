{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596060570272",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "import pystaticplot as ps\n",
    "obj = ps.dataviz()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection(filepath):\n",
    "\n",
    "    # lê o dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # monta um dataframe com todas as features e a porcentagem dos dados nulos\n",
    "    null_data = pd.DataFrame(data = [list(df.columns), list(df.isnull().sum())]).transpose()\n",
    "    null_data.columns = ['feature', 'null_data']\n",
    "\n",
    "    # filtra o dataframe para colunas com no máximo 50% dos dados ausentes\n",
    "    df = df.filter(items = null_data.feature.loc[null_data.null_data < 0.5 * df.shape[0]])\n",
    "\n",
    "    # exclui as colunas que não serão usadas na análise\n",
    "    df.drop(columns = ['Unnamed: 0', 'fl_matriz','natureza_juridica_macro','de_ramo','fl_spa', 'fl_antt',\n",
    "    'idade_empresa_anos','vl_total_veiculos_pesados_grupo','vl_total_veiculos_leves_grupo','fl_veiculo',\n",
    "    'fl_me','fl_sa','fl_epp','fl_mei','fl_ltda','dt_situacao','fl_st_especial','fl_email','fl_telefone',\n",
    "    'fl_rm','nm_divisao','fl_optante_simples','sg_uf_matriz','de_saude_tributaria','de_saude_rescencia',\n",
    "    'nu_meses_rescencia','fl_simples_irregular','empsetorcensitariofaixarendapopulacao','nm_meso_regiao',\n",
    "    'nm_micro_regiao','fl_passivel_iss','idade_media_socios','idade_maxima_socios','idade_minima_socios',\n",
    "    'qt_socios_st_regular','de_faixa_faturamento_estimado','vl_faturamento_estimado_grupo_aux',\n",
    "    'vl_faturamento_estimado_aux','qt_socios','qt_socios_pj','qt_socios_pf', 'qt_filiais','fl_optante_simei'], axis = 1, inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "\n",
    "    # realiza o filtro para selecionar somente as features que serão utilizadas no modelo\n",
    "    df = df.filter(items = ['id','de_natureza_juridica','sg_uf', 'setor', 'nm_segmento', 'idade_emp_cat','de_nivel_atividade',\n",
    "    'de_faixa_faturamento_estimado_grupo'])\n",
    "\n",
    "    # todos os setores que estão nulos serão classificados na nova categoria OUTROS\n",
    "    df.setor.fillna('OUTROS', inplace = True)\n",
    "\n",
    "    # todos os segmentos que estão nulos serão classificados na nova categoria OUTROS\n",
    "    df.nm_segmento.fillna('OUTROS', inplace =  True)\n",
    "\n",
    "    # para os dados que estiverem com dado nulo será inserido a categoria que mais se repete\n",
    "    df.de_faixa_faturamento_estimado_grupo.fillna(df.de_faixa_faturamento_estimado_grupo.mode().values[0], inplace = True)\n",
    "\n",
    "    # para os dados que estiverem com dado nulo será inserido a categoria que mais se repete, porém será realizado um group \n",
    "    # by a partir do nível de ativade da empresa\n",
    "    faixa_faturamento_por_nivel = df.groupby(['de_faixa_faturamento_estimado_grupo'])['de_nivel_atividade'].agg(pd.Series.mode)\n",
    "    df.de_nivel_atividade.fillna(df.de_faixa_faturamento_estimado_grupo.map(faixa_faturamento_por_nivel), inplace = True)\n",
    "\n",
    "    df_return = df.copy()\n",
    "\n",
    "    # transforma as variáveis categóricas em variáveis discretas\n",
    "    labelencoder = LabelEncoder()\n",
    "    df.de_natureza_juridica = labelencoder.fit_transform(df.de_natureza_juridica)\n",
    "    df.sg_uf = labelencoder.fit_transform(df.sg_uf)\n",
    "    df.setor = labelencoder.fit_transform(df.setor)\n",
    "    df.nm_segmento = labelencoder.fit_transform(df.nm_segmento)\n",
    "    df.idade_emp_cat = labelencoder.fit_transform(df.idade_emp_cat)\n",
    "    df.de_nivel_atividade = labelencoder.fit_transform(df.de_nivel_atividade)\n",
    "    df.de_faixa_faturamento_estimado_grupo = labelencoder.fit_transform(df.de_faixa_faturamento_estimado_grupo)\n",
    "\n",
    "    # cria a classe para padronizar o dataset\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # separa os id's\n",
    "    ids = df.id\n",
    "\n",
    "    # separa as features\n",
    "    features = df.filter(items = ['de_natureza_juridica','sg_uf', 'setor', 'nm_segmento', 'idade_emp_cat','de_nivel_atividade',\n",
    "    'de_faixa_faturamento_estimado_grupo'])\n",
    "\n",
    "    # padroniza os dados\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # redução de dimensionalidade utilizando método PCA - Análise das Componentes Principais para 3 dimensões\n",
    "    pca = PCA(n_components= 3)\n",
    "    features = pca.fit_transform(features)\n",
    "\n",
    "    return df_return, ids, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features):\n",
    "\n",
    "    # gera o modelo de KMeans\n",
    "    kmeans = KMeans(n_clusters= 6, init = 'k-means++')\n",
    "\n",
    "    # treina o modelo\n",
    "    kmeans.fit(features)\n",
    "\n",
    "    # pega os valores dos centróides\n",
    "    centroides = kmeans.cluster_centers_\n",
    "\n",
    "    # pega as distancias para o centróides\n",
    "    distancia = kmeans.fit_transform(features)\n",
    "\n",
    "    # pega o agrupamento de cada id\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # salva o modelo\n",
    "    joblib.dump(kmeans, 'model.pkl')\n",
    "    \n",
    "    return  kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, kmeans):\n",
    "\n",
    "    # salva o dataset pre processado\n",
    "    df['label'] = kmeans.labels_\n",
    "    df.to_csv('../../market_preprocessing.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "\n",
    "    # carrega modelo\n",
    "    model = joblib.load(filepath)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(kmeans, features):\n",
    "\n",
    "    # realiza a previsão com base no modelo\n",
    "    output = kmeans.predict(features)\n",
    "\n",
    "    return list(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio(filepath, market):\n",
    "\n",
    "    # lê o portfolio e o banco de informações com as labels \n",
    "    portfolio = pd.read_csv(filepath)\n",
    "    market_label = pd.read_csv(market)\n",
    "    portfolio = portfolio.id\n",
    "\n",
    "    # verifica quais as labels dos ids que estão no portfolio\n",
    "    df = market_label[market_label['id'].isin(portfolio)]  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_collection('../../estaticos_market.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, ids, features = data_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(df, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(kmeans, [[0.5, 1.87, -1.28]])\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.pkl')\n",
    "output1 = predict(model, [[0.5, 1.87, -1.28]])\n",
    "output1[0]"
   ]
  }
 ]
}